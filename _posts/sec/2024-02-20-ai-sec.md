---
layout:		post
category:	"sec"
title:		"AI人工智能安全"

tags:		[]
---
- Content
{:toc}




当前，人工智能发展再一次迈入关键时期，以生成式人工智能为代表的新技术、新应用不断打破人们对于人工智能的固有认知，也带来了大量网络意识形态安全、数据安全、个人信息安全等方面新风险、新挑战，化解安全风险、统筹发展和安全成为重大难题。

# 应用领域

2020年后，当**自动驾驶**、**人脸识别**等热门应用发展逐渐放缓、社会对人工智能整体发展预期日益冷静时，大模型技术潜力的释放以最振聋发聩的方式宣告了人工智能第三次高速发展期远未结束，当前正是攀登发展高峰的关键时期。

另一方面，当人工智能可以通过人类最严格的**考试**、同时执行多种**工作命令**、具备一定的**推理规划**能力、生成以假乱真的照片、**模仿**人类与人聊天不被发现时，其安全问题也更为复杂棘手，传统安全考虑以及管理方法需要重新审视。

当前，人工智能在多个行业领域广泛应用，在**制造领域**的运营管理优化、制造过程优化等环节，**智能家居**领域的身份鉴别、功能控制、安全防护等环节，**智能交通**领域的动态感知、自动驾驶、车路协同等方面，**智能医疗**领域的辅助诊断、治疗监护、疫情防控等方面，**教育**领域的虚拟实验室、虚拟教室、课件制作、智能判卷、教学效果分析等方面，**金融**领域的金融风险控制等方面，都推动了相关产品服务的新一轮变革。

人工智能的发展不仅颠覆了数字内容生产方式、处理方式和消费模式，而且极大丰富了人们的**数字生活**，虚拟试装增加购物体验、虚拟主播增强广告效果、智能客服提升反馈效率、虚拟教师增强师生交互、智能办公助手提高各类文档的撰写效率、智能编程助手降低编程时间与人力成本、智能翻译降低沟通壁垒，人工智能应用已成为人类生产生活中必不可少的电子助手。

# 安全属性

1、可靠性：指人工智能及其所在系统在承受不利环境或意外变化时，例如数据变化、噪声、干扰等因素，仍能按照既定的目标运行、保持结果有效的特性。可靠性通常需要综合考虑系统的容错性、恢复性、健壮性等多个方面。

2、透明性：指人工智能在设计、训练、测试、部署过程中保持可见、可控的特性，只有具备了透明性，用户才能够在必要时获取模型有关信息，包括模型结构、参数、输入输出等，方可进一步实现人工智能开发过程的可审计以及可追溯。

3、可解释性：描述了人工智能算法模型可被人理解其运行逻辑的特性。具备可解释性的人工智能，其计算过程中使用的数据、算法、参数和逻辑等对输出结果的影响能够被人类理解，使人工智能更易于被人类管控、更容易被社会接受。

4、公平性：指人工智能模型在进行决策时，不偏向某个特定的个体或群体，也不歧视某个特定的个体或群体，平等对待不同性别、不同种族、不同文化背景的人群，保证处理结果的公正、中立，不引入偏见和歧视因素。

5、隐私性：指人工智能在开发与运行的过程中实现了保护隐私的特性，包括对个人信息和个人隐私的保护、对商业秘密的保护等。隐私性旨在保障个人和组织的合法隐私权益，常见的隐私增强方案包括最小化数据处理范围、个人信息匿名化处理、数据加密和访问控制等。

# 安全风险

1. 用户数据用于训练，放大隐私信息泄露风险：收集个人信息、个人隐私、商业秘密、科研成果等
2. 算法模型日趋复杂，可解释性目标难实现：难归因，不可解释
3. 可靠性问题仍然制约人工智能关键领域应用：模型安全性、可靠性、不可全部覆盖性
4. 滥用误用人工智能，扰乱生产生活安全秩序：虚假，以假乱真
5. 模型和数据成为核心资产，安全保护难度提升
6. 网络意识形态安全面临新风险：内容安全





# 参考标准

见《人工智能安全标准化白皮书（2023版）》附录部分。请注意，这些信息是基于提供的文件内容，实际发布情况可能会有变动。



已经发布的相关标准包括：

1. GB/T 38542-2020《信息安全技术 基于生物特征识别的移动智能终端身份鉴别技术框架》
2. GB/T 38671-2020《信息安全技术 远程人脸识别系统技术要求》
3. GB/T 40660-2021《信息安全技术 生物特征识别信息保护基本要求》
4. GB/T 4181-2022《信息安全技术 人脸识别数据安全要求》
5. GB/T 41807-2022《信息安全技术 声纹识别数据安全要求》
6. GB/T 41806-2022《信息安全技术 基因识别数据安全要求》
7. GB/T 41773-2022《信息安全技术 步态识别数据安全要求》
8. GB/T 41871-2022《信息安全技术 汽车数据处理安全要求》
9. GB/T 41815.1-2022《信息技术 生物特征识别呈现攻击检测 第1部分：框架》
10. GB/T 41815.2-2022《信息技术 生物特征识别呈现攻击检测 第2部分：数据格式》
11. GB/T 41815.3-2023《信息技术 生物特征识别呈现攻击检测 第3部分：测试与报告》
12. GB/T 37036.3-2019《信息技术 移动设备生物特征识别 第3部分：人脸》
13. GB/T 37036.8-2022《信息技术 移动设备生物特征识别 第8部分：呈现攻击检测》
14. GB/T 5271.37-2021《信息技术 词汇 第37部分：生物特征识别》
15. GB/T 40660-2021《信息安全技术 生物特征识别信息保护基本要求》
16. GB/T 4181-2022《信息安全技术 人脸识别数据安全要求》
17. GB/T 41807-2022《信息安全技术 声纹识别数据安全要求》
18. GB/T 41806-2022《信息安全技术 基因识别数据安全要求》
19. GB/T 41773-2022《信息安全技术 步态识别数据安全要求》
20. GB/T 41871-2022《信息安全技术 汽车数据处理安全要求》
21. GB/T 41815.1-2022《信息技术 生物特征识别呈现攻击检测 第1部分：框架》
22. GB/T 41815.2-2022《信息技术 生物特征识别呈现攻击检测 第2部分：数据格式》
23. GB/T 41815.3-2023《信息技术 生物特征识别呈现攻击检测 第3部分：测试与报告》
24. GB/T 37036.3-2019《信息技术 移动设备生物特征识别 第3部分：人脸》
25. GB/T 37036.8-2022《信息技术 移动设备生物特征识别 第8部分：呈现攻击检测》
26. GB/T 5271.37-2021《信息技术 词汇 第37部分：生物特征识别》
27. T/CESA 1193-2022《信息技术 人工智能 风险管理能力评估》
28. T/AI 110.1-2020《人工智能视觉隐私保护 第1部分：通用技术要求》
29. T/AI 110.2-2022《人工智能视觉隐私保护 第2部分：技术应用指南》
30. T/AI 113-2021《生物特征识别服务中的隐私保护技术指南》
31. T/AI 111-2020《生物特征模板的安全使用要求》
32. T/AI 1205-2023《信息技术 数字视网膜系统 第11部分：安全与隐私保护》

即将发布的相关标准包括：

1. GB/T 20211000-T-469《信息安全技术 机器学习算法安全评估规范》（报批稿）
2. GB/T 20230249-T-469《信息安全技术 人工智能计算平台安全框架》（征求意见稿）
3. GB/T 20221791-T-469《人工智能 管理体系》（立项）
4. GB/T 20230253-T-469《信息安全技术 基于个人信息的自动化决策安全要求》（立项）
5. GB/T 20221220-T-469《信息技术 生物特征识别 人脸识别 系统应用要求》（立项）
6. YD/T 4087-2022《移动智能终端人脸识别安全技术要求及测试评估方法》（发布）
7. 2023-0041TYD《人工智能开发平台通用能力要求 第2部分：安全要求》（立项）
8. 2023-0039TYD《面向人脸识别系统的人脸信息保护基础能力要求》（立项）
9. 人脸识别线下支付安全要求（草案）
10. 电信网和互联网人脸识别数据安全检测要求（立项）
11. 人工智能数据通用安全要求（征求意见稿）
12. 人脸识别分级分类应用标准（草案）

# 安全问题

**网络安全**

围绕人工智能服务过程中，可能会面临的对抗样本攻击、爬山攻击、模型窃取、供应链攻击等新型攻击威胁，需要研究在数据集防护、算法模型保护、抗逆向攻击等方面的安全技术措施指南，帮助人工智能服务提供者保护业务数据以及人工智能模型参数等的机密性和完整性。



**数据安全**

在人工智能通常收集用户输入数据用于训练的背景下，如何保障用户输入数据的安全亟需技术标准。根据操作场景的不同，用户输入的数据可能包含人脸、身份证号、家庭住址等个人信息，可能包括个人健康情况、情感状况等个人隐私，可能包括企业技术和经营活动有关的商业秘密，甚至可能包括国家秘密等。需要落实数据安全法》《个人信息保护法》等法律法规，提出可以切实解决用户输入数据安全问题的相关标准规范。



**内容安全**

人工智能的目标是模拟、扩展和延伸人类智能，如果人工智能只是单纯追求统计最优解，可能表现得不那么有“人性”；相反，包含一些人类政治、伦理、道德等方面观念的人工智能会表现得更像人、更容易被人所接受。事实上，为了解决人工智能面对敏感复杂问题的表现，开发者通常将包含着开发者所认为正确观念的答案加入训练过程，并通过强化学习等方式输入到模型中，当模型掌握了这些观念时，能够产生更能被人接受的回答。然而，由于政治、伦理、道德等复杂问题往往没有全世界通用的标准答案，符合某一区域、人群观念判断的人工智能，可能会与另一区域、人群在政治、伦理、道德等方面有较大差异。因此，使用内嵌了违背我国社会共识以及公序良俗的人工智能，可能对我国网络意识形态安全造成冲击。



人工智能安全可能存在以下安全问题：

- 网络安全风险：包括网络设施和学习框架的漏洞、后门安全问题，以及人工智能技术恶意应用导致的系统网络安全风险。

- 数据安全风险：涉及人工智能系统中的训练数据偏差、非授权篡改以及隐私数据泄露等安全风险。

- 算法安全风险：包括算法设计或实施错误导致的不符合预期或伤害性结果，算法潜藏偏见和歧视，以及算法黑箱问题导致的决策不可解释性。

- 信息安全风险：涉及人工智能技术应用于信息传播以及人工智能产品和应用输出的信息内容安全问题。

- 社会安全风险：包括人工智能产业化应用带来的结构性失业、对社会伦理道德的冲击，以及可能给个人人身安全带来的损害。

- 国家安全风险：人工智能在军事作战、社会舆情等领域应用可能给国家军事安全和政体安全带来的风险隐患。

- 技术不成熟性：人工智能技术本身可能存在的不成熟性，如算法的不可解释性、数据强依赖性等，可能导致安全风险。

- 恶意应用：人工智能技术可能被恶意利用，用于网络攻击、数据窃取、虚假信息传播等非法活动。

- 法律和伦理挑战：人工智能的发展可能引发法律和伦理问题，如责任归属、隐私保护、算法歧视等。

- 技术监管难度：随着人工智能技术的快速发展，监管和评估其安全性的难度增加，需要建立有效的监管体系和评估机制。



# 参考

- [白宫成立人工智能安全联盟](https://mp.weixin.qq.com/s/Iv1fnG39xzn2QhLmI3EATw)
- [《人工智能安全标准化白皮书（2023版）》发布 - 全国信息安全标准化技术委员会](https://www.tc260.org.cn/front/postDetail.html?id=20230531105159)
- [CSA大中华区发布《AI安全白皮书》，中国电信、蚂蚁集团、华为、百度安全等单位参编](https://c-csa.cn/about/news-detail/i-1045.html)
